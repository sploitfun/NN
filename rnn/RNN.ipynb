{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import Packages\n",
    "\n",
    "#To build a NN model\n",
    "from keras.models import Sequential\n",
    "\n",
    "#To build a densely connected NN layer\n",
    "from keras.layers import Dense\n",
    "\n",
    "#Regularization Function\n",
    "from keras.layers import Dropout\n",
    "\n",
    "#Activation Function\n",
    "from keras.layers import Activation\n",
    "\n",
    "#To build a fully connected RNN\n",
    "from keras.layers import SimpleRNN\n",
    "\n",
    "#Optimizer\n",
    "from keras.optimizers import Adagrad\n",
    "\n",
    "#To save keras model\n",
    "from keras.models import load_model\n",
    "\n",
    "#Math ops\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters: 217649, Unique Characters: 94\n"
     ]
    }
   ],
   "source": [
    "#Fetching Dataset\n",
    "data = ''\n",
    "files = os.listdir('../data/js/Basics/')\n",
    "for file in files:\n",
    "        data += open('../data/js/Basics/'+file,'r').read()\n",
    "\n",
    "#Unique Characters\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print 'Total Characters: %d, Unique Characters: %d' % (data_size, vocab_size)\n",
    "\n",
    "#Char-to_Index and Index-to-Char dicts\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load saved model, if exists\n",
    "if os.path.isfile('../dump/char-rnn-model.h5'):\n",
    "    model = load_model('../dump/char-rnn-model.h5')\n",
    "else:\n",
    "    model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Data Preprocessing\n",
    "#Here we are preparing our data to model many-to-one model ie) our unrolled RNN contains many inputs and only one output  \n",
    "time_steps = 30\n",
    "if model is None:\n",
    "    p = 0\n",
    "    #Each list element contains time_steps characters of data\n",
    "    inputs = []\n",
    "    #Each list element contains corresponding character found at offset time_steps from data element\n",
    "    targets = []\n",
    "    while p+time_steps+1 <= data_size:\n",
    "        #Convert time_steps characters into its integer representation\n",
    "        inputs.append([char_to_ix[ch] for ch in data[p:p+time_steps]])\n",
    "        #Convert target character into its integer representation\n",
    "        targets.append(char_to_ix[data[p+time_steps]])\n",
    "        p = p + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    #Convert data into a representation which is suited for our NN model\n",
    "    #Here we are using one-hot vector representation which is an N dimension vector of all zeros except for a 1 at a corresponding integer index\n",
    "    X = np.zeros((len(inputs), time_steps, vocab_size))\n",
    "    y = np.zeros((len(targets), vocab_size))\n",
    "    for i, inp in enumerate(inputs):\n",
    "        for t, inp_t in enumerate(inp):\n",
    "            X[i, t, inp_t] = 1\n",
    "    for i, tgt in enumerate(targets):\n",
    "            y[i, tgt] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_2 (SimpleRNN)     (None, 100)               19500     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 94)                9494      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 94)                0         \n",
      "=================================================================\n",
      "Total params: 28,994\n",
      "Trainable params: 28,994\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "    #Build NN model\n",
    "    model = Sequential()\n",
    "    #Build RNN model\n",
    "    #In this example\n",
    "        #Input Layer contains time_steps * vocab_size of data ie) each row contains input at a particular time step \n",
    "        #and that input contains vocab_size of data in one-hot vector representation format\n",
    "        #Hidden layer at each time step contains 100 (arbitrarily chosen) hidden units\n",
    "        #Apply tanh activation function in hidden layer. \n",
    "    model.add(SimpleRNN(100, input_shape=(time_steps, vocab_size), activation='tanh'))\n",
    "    \n",
    "    #Dropout helps to prevent overfitting\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    #Using Dense add output layer which contains vocab_size output units - one per each unique character\n",
    "    model.add(Dense(vocab_size))\n",
    "    #Apply softmax activation function in output layer\n",
    "    model.add(Activation('softmax'))\n",
    "    model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Model Parameters:** In our example total paramters is **28994**. Lets see how is this calculated.\n",
    "\n",
    "Total Params = 28994 = 19500 + 9494\n",
    "\n",
    "**19500** = (94 * 100) + (100 * 100) + 100\n",
    "\n",
    "where 94 = Input Units, 100 = Hidden Units, 100 = Hidden Units, 100 = Hidden Units and 100 = Bias Units\n",
    "\n",
    "**9494** = (100 * 94) + 94\n",
    "\n",
    "where 100 = Hidden Units, 94 = Output Units and 94 = Bias Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    #Optimizer\n",
    "    optimizer = Adagrad(lr=0.01)\n",
    "    #Configure the learning process.\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "217619/217619 [==============================] - 65s - loss: 1.7312    \n",
      "Epoch 2/10\n",
      "217619/217619 [==============================] - 60s - loss: 1.4308    \n",
      "Epoch 3/10\n",
      "217619/217619 [==============================] - 61s - loss: 1.3425    \n",
      "Epoch 4/10\n",
      "217619/217619 [==============================] - 61s - loss: 1.2903    \n",
      "Epoch 5/10\n",
      "217619/217619 [==============================] - 61s - loss: 1.2530    \n",
      "Epoch 6/10\n",
      "217619/217619 [==============================] - 61s - loss: 1.2252    \n",
      "Epoch 7/10\n",
      "217619/217619 [==============================] - 61s - loss: 1.2033    \n",
      "Epoch 8/10\n",
      "217619/217619 [==============================] - 61s - loss: 1.1858    \n",
      "Epoch 9/10\n",
      "217619/217619 [==============================] - 60s - loss: 1.1706    \n",
      "Epoch 10/10\n",
      "217619/217619 [==============================] - 59s - loss: 1.1584    \n"
     ]
    }
   ],
   "source": [
    "    #Train the RNN model for 10 epochs and after training the model with a batch_size of 32 training samples, perform parameter update.\n",
    "    model.fit(X, y, batch_size=32, epochs=10)\n",
    "    #Save the trained model\n",
    "    model.save('../dump/char-rnn-model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Seed:\n",
      "testProperty(\"nextSibling\");\n",
      "Generated Sentence: \n",
      "t e s t P r o p e r t y ( \" s e r i n t \" ) ; \n",
      "t e s t P r o p e r t y ( \" c r i n g e t e t e \" ) ; \n",
      "t e s t P r o p e r t y ( \" c o m a t e \" ) ; \n",
      "t e s t P r o p e r t y ( \" c o m a t e \" ) ; \n",
      "t e s t P r o p e r t y ( \" c o m a t e \" ) ; \n",
      "t e s t P r o p e r t y ( \" c o m a t e \" ) ; \n",
      "t e s t P r o p e r t y ( \" c o m a t e \" ) ; \n",
      "t e s t P r o p e r t y ( \" c o m a t e\n"
     ]
    }
   ],
   "source": [
    "#Sequence Generation\n",
    "for i in range(0,1):\n",
    "    start_index = random.randint(0, data_size - time_steps - 1)\n",
    "    idx = data.find('\\n', start_index)\n",
    "    if idx!=-1 and idx+time_steps <= data_size:\n",
    "            sentence = data[idx: idx + time_steps]\n",
    "    else:\n",
    "        break\n",
    "    sentence = data[idx: idx + time_steps]\n",
    "    print '\\nSeed:' + sentence\n",
    "    print 'Generated Sentence:',\n",
    "    #Generate next 200 characters for the given seed\n",
    "    for i in range(200):\n",
    "        x = np.zeros((1, time_steps, vocab_size))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char_to_ix[char]] = 1.\n",
    "        #Predict the target for the given sentence\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        #Find the index of the character whose probability is maximum\n",
    "        next_index = np.argmax(preds)\n",
    "        next_char = ix_to_char[next_index]\n",
    "        print next_char,\n",
    "        #Move input by one character\n",
    "        sentence = sentence[1:] + next_char\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "    1. https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "    2. https://github.com/Microsoft/ChakraCore/tree/master/test/Basics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
